{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### CBOW = Continuous Bag of Words\n#### Our objective: to obtain vector embeddings of 20,000 common english language words by training for their contextual meaning.\n#### AKA: subpar version of Word2Vec","metadata":{}},{"cell_type":"code","source":"import gensim.downloader as api\nimport tensorflow\n\ndataset = api.load(\"text8\")","metadata":{"id":"EznoYWdlEp4d","execution":{"iopub.status.busy":"2023-06-13T10:36:45.952813Z","iopub.execute_input":"2023-06-13T10:36:45.953212Z","iopub.status.idle":"2023-06-13T10:36:46.110507Z","shell.execute_reply.started":"2023-06-13T10:36:45.953183Z","shell.execute_reply":"2023-06-13T10:36:46.109391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer","metadata":{"id":"Qd_se-m4Ete_","execution":{"iopub.status.busy":"2023-06-13T10:36:46.113207Z","iopub.execute_input":"2023-06-13T10:36:46.113984Z","iopub.status.idle":"2023-06-13T10:36:46.119559Z","shell.execute_reply.started":"2023-06-13T10:36:46.113938Z","shell.execute_reply":"2023-06-13T10:36:46.118372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = 20000\ntokenizer = Tokenizer(num_words=vocab_size)\ntokenizer.fit_on_texts(dataset)\nsequences = tokenizer.texts_to_sequences(dataset)\n\ntokenizer.word_index","metadata":{"id":"6ZR2dHqCFgBp","outputId":"f76b7395-0197-442c-b7c1-e16212e64b8d","execution":{"iopub.status.busy":"2023-06-13T10:36:46.121284Z","iopub.execute_input":"2023-06-13T10:36:46.121725Z","iopub.status.idle":"2023-06-13T10:37:14.261766Z","shell.execute_reply.started":"2023-06-13T10:36:46.121697Z","shell.execute_reply":"2023-06-13T10:37:14.260745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras.layers import Dense, Input, Embedding, Lambda\nfrom tensorflow.keras.models import Model","metadata":{"id":"ST8VUl8CGG2I","execution":{"iopub.status.busy":"2023-06-13T10:37:14.264798Z","iopub.execute_input":"2023-06-13T10:37:14.265327Z","iopub.status.idle":"2023-06-13T10:37:14.271312Z","shell.execute_reply.started":"2023-06-13T10:37:14.265288Z","shell.execute_reply":"2023-06-13T10:37:14.270502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nrandom.seed(1)\nnp.random.seed(1)\ntf.random.set_seed(1)","metadata":{"id":"cT8B5tP2GHg4","execution":{"iopub.status.busy":"2023-06-13T10:37:14.272577Z","iopub.execute_input":"2023-06-13T10:37:14.273523Z","iopub.status.idle":"2023-06-13T10:37:14.286782Z","shell.execute_reply.started":"2023-06-13T10:37:14.273489Z","shell.execute_reply":"2023-06-13T10:37:14.285587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context_size = 10\nembedding_dim = 50\n\ni = Input(shape=(context_size,))\nx = Embedding(vocab_size, embedding_dim)(i)\nx = Lambda(lambda t: tf.reduce_mean(t, axis=1))(x)\nx = Dense(vocab_size, activation='softmax')(x)\n\nmodel = Model(i, x)","metadata":{"id":"4Y8TjYNkGNKp","execution":{"iopub.status.busy":"2023-06-13T10:37:14.289335Z","iopub.execute_input":"2023-06-13T10:37:14.289666Z","iopub.status.idle":"2023-06-13T10:37:14.374850Z","shell.execute_reply.started":"2023-06-13T10:37:14.289638Z","shell.execute_reply":"2023-06-13T10:37:14.374068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"id":"unQBRLvtJEx0","outputId":"4649d1c5-0bf7-4d7b-da7d-61a7081d5af2","execution":{"iopub.status.busy":"2023-06-13T10:37:14.375963Z","iopub.execute_input":"2023-06-13T10:37:14.376695Z","iopub.status.idle":"2023-06-13T10:37:14.398522Z","shell.execute_reply.started":"2023-06-13T10:37:14.376663Z","shell.execute_reply":"2023-06-13T10:37:14.397549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n              optimizer='adam',\n              metrics=['accuracy'])","metadata":{"id":"NRhy_KUXJGeb","execution":{"iopub.status.busy":"2023-06-13T10:37:14.399993Z","iopub.execute_input":"2023-06-13T10:37:14.400333Z","iopub.status.idle":"2023-06-13T10:37:14.414873Z","shell.execute_reply.started":"2023-06-13T10:37:14.400299Z","shell.execute_reply":"2023-06-13T10:37:14.413869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"half_context_size = context_size // 2\n\ndef data_generator(sequences, batch_size=128):\n  X_batch = np.zeros((batch_size, context_size))\n  Y_batch = np.zeros(batch_size)\n  n_batches = int(np.ceil(len(sequences) / batch_size))\n\n  while True:\n    random.shuffle(sequences)\n\n    # one epoch will be one pass through the data\n    for i in range(n_batches):\n      batch_sequences = sequences[i * batch_size:(i + 1) * batch_size]\n\n      current_batch_size = len(batch_sequences) # may be less than batch_size\n      for ii in range(current_batch_size):\n        seq = batch_sequences[ii]\n        j = np.random.randint(0, len(seq) - context_size - 1)\n        x1 = seq[j:j + half_context_size]\n        x2 = seq[j + half_context_size + 1:j + context_size + 1]\n        # x = x1 + x2\n        # X_batch[ii] = x\n        X_batch[ii, :half_context_size] = x1\n        X_batch[ii, half_context_size:] = x2\n        y = seq[j + half_context_size]\n        Y_batch[ii] = y\n\n      yield X_batch[:current_batch_size], Y_batch[:current_batch_size]","metadata":{"id":"hcns3gxVKQh9","execution":{"iopub.status.busy":"2023-06-13T10:37:14.416153Z","iopub.execute_input":"2023-06-13T10:37:14.416445Z","iopub.status.idle":"2023-06-13T10:37:14.425601Z","shell.execute_reply.started":"2023-06-13T10:37:14.416421Z","shell.execute_reply":"2023-06-13T10:37:14.424540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 128\nr = model.fit(\n  data_generator(sequences, batch_size),\n  epochs=10000,\n  steps_per_epoch=int(np.ceil(len(sequences) / batch_size))\n)","metadata":{"id":"Cb0N8mbTLfTw","outputId":"f45366ba-7f27-4e3f-ea19-651acd4d9fe7","execution":{"iopub.status.busy":"2023-06-13T10:37:14.429954Z","iopub.execute_input":"2023-06-13T10:37:14.430396Z","iopub.status.idle":"2023-06-13T10:40:59.930882Z","shell.execute_reply.started":"2023-06-13T10:37:14.430362Z","shell.execute_reply":"2023-06-13T10:40:59.929261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(r.history['loss'], label='loss')\nplt.legend();","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:40:59.932031Z","iopub.status.idle":"2023-06-13T10:40:59.932492Z","shell.execute_reply.started":"2023-06-13T10:40:59.932296Z","shell.execute_reply":"2023-06-13T10:40:59.932316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(r.history['accuracy'], label='acc')\nplt.legend();","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:40:59.933583Z","iopub.status.idle":"2023-06-13T10:40:59.933968Z","shell.execute_reply.started":"2023-06-13T10:40:59.933789Z","shell.execute_reply":"2023-06-13T10:40:59.933807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings = model.layers[1].get_weights()[0]\nembeddings","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:40:59.935591Z","iopub.status.idle":"2023-06-13T10:40:59.936039Z","shell.execute_reply.started":"2023-06-13T10:40:59.935842Z","shell.execute_reply":"2023-06-13T10:40:59.935861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import NearestNeighbors\nneighbors = NearestNeighbors(n_neighbors=5, algorithm='ball_tree')\nneighbors.fit(embeddings)","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:40:59.937128Z","iopub.status.idle":"2023-06-13T10:40:59.937501Z","shell.execute_reply.started":"2023-06-13T10:40:59.937328Z","shell.execute_reply":"2023-06-13T10:40:59.937345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_neighbors(query):\n  query_idx = tokenizer.word_index[query]\n  query = embeddings[query_idx:query_idx + 1]\n  distances, indices = neighbors.kneighbors(query)\n  for idx in indices[0]:\n    word = tokenizer.index_word[idx]\n    print(word)","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:40:59.938512Z","iopub.status.idle":"2023-06-13T10:40:59.939185Z","shell.execute_reply.started":"2023-06-13T10:40:59.938954Z","shell.execute_reply":"2023-06-13T10:40:59.938975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_neighbors('uncle')","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:40:59.940178Z","iopub.status.idle":"2023-06-13T10:40:59.940866Z","shell.execute_reply.started":"2023-06-13T10:40:59.940660Z","shell.execute_reply":"2023-06-13T10:40:59.940680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_neighbors('paris')","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:40:59.942162Z","iopub.status.idle":"2023-06-13T10:40:59.942552Z","shell.execute_reply.started":"2023-06-13T10:40:59.942372Z","shell.execute_reply":"2023-06-13T10:40:59.942390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_embedding(word):\n  idx = tokenizer.word_index[word]\n  return embeddings[idx:idx + 1]\n\nengland = get_embedding('england')\n\nenglish = get_embedding('english')\naustralian = get_embedding('australian')\n\n# australia - australian = england - english in resulting embedding space\n# expected query = australia\nquery = england - english + australian\n\ndistances, indices = neighbors.kneighbors(query)\nfor idx in indices[0]:\n  word = tokenizer.index_word[idx]\n  print(word)","metadata":{"execution":{"iopub.status.busy":"2023-06-13T10:40:59.943534Z","iopub.status.idle":"2023-06-13T10:40:59.945010Z","shell.execute_reply.started":"2023-06-13T10:40:59.944761Z","shell.execute_reply":"2023-06-13T10:40:59.944786Z"},"trusted":true},"execution_count":null,"outputs":[]}]}